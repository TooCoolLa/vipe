#!/usr/bin/env python3
"""
åˆ†æç›¸æœºä½å§¿è½¨è¿¹ä¸­çš„çªè·³ï¼ˆJumpï¼‰

ç”¨æ³•:
    python analyze_jumps.py <pose_file.json> [options]

ç¤ºä¾‹:
    python analyze_jumps.py data1/camera_poses.json
    python analyze_jumps.py data1/camera_poses_fused.json --threshold 3.5
    python analyze_jumps.py data1/camera_poses.json --output jumps_report.txt
"""

import json
import argparse
import numpy as np
from pathlib import Path
from collections import defaultdict
from typing import List, Dict, Tuple


class JumpAnalyzer:
    """çªè·³åˆ†æå™¨"""
    
    def __init__(self, pose_file: Path, sigma_multiplier: float = 3.0):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            pose_file: camera_poses.jsonæ–‡ä»¶è·¯å¾„
            sigma_multiplier: çªè·³é˜ˆå€¼çš„æ ‡å‡†å·®å€æ•°ï¼ˆé»˜è®¤3Ïƒï¼‰
        """
        self.pose_file = pose_file
        self.sigma_multiplier = sigma_multiplier
        
        # åŠ è½½æ•°æ®
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {pose_file}")
        with open(pose_file, 'r') as f:
            self.data = json.load(f)
        
        # æå–ä½ç½®
        self.positions = self._extract_positions()
        
        # è®¡ç®—å¸§é—´è·ç¦»
        self.distances = np.linalg.norm(np.diff(self.positions, axis=0), axis=1)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self.mean_dist = np.mean(self.distances)
        self.std_dist = np.std(self.distances)
        self.threshold = self.mean_dist + sigma_multiplier * self.std_dist
        
        # è¯†åˆ«çªè·³
        self.jump_mask = self.distances > self.threshold
        self.jump_indices = np.where(self.jump_mask)[0]
        
        print(f"âœ… åŠ è½½å®Œæˆ: {len(self.positions)} å¸§")
        print(f"ğŸ“Š å¹³å‡å¸§é—´è·: {self.mean_dist:.4f}m, æ ‡å‡†å·®: {self.std_dist:.4f}m")
        print(f"ğŸ¯ çªè·³é˜ˆå€¼: {self.threshold:.4f}m (å‡å€¼ + {sigma_multiplier}Ïƒ)")
        print(f"âš ï¸  å‘ç° {len(self.jump_indices)} ä¸ªçªè·³ ({len(self.jump_indices)/len(self.distances)*100:.2f}%)\n")
    
    def _extract_positions(self) -> np.ndarray:
        """ä»poseæ•°æ®ä¸­æå–ä½ç½®"""
        positions = []
        for frame in self.data:
            matrix = np.array(frame['matrix'])
            pos = matrix[:3, 3]
            positions.append(pos)
        return np.array(positions)
    
    def group_consecutive_jumps(self, gap: int = 50) -> List[List[int]]:
        """
        å°†è¿ç»­æˆ–æ¥è¿‘çš„çªè·³åˆ†ç»„
        
        Args:
            gap: è®¤ä¸ºæ˜¯è¿ç»­çš„æœ€å¤§å¸§é—´éš”
            
        Returns:
            çªè·³ç»„åˆ—è¡¨ï¼Œæ¯ç»„æ˜¯å¸§ç´¢å¼•åˆ—è¡¨
        """
        if len(self.jump_indices) == 0:
            return []
        
        groups = []
        current_group = [self.jump_indices[0]]
        
        for idx in self.jump_indices[1:]:
            if idx - current_group[-1] <= gap:
                current_group.append(idx)
            else:
                groups.append(current_group)
                current_group = [idx]
        groups.append(current_group)
        
        return groups
    
    def analyze_jump_group(self, group: List[int]) -> Dict:
        """
        åˆ†æå•ä¸ªçªè·³ç»„çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            group: çªè·³å¸§ç´¢å¼•åˆ—è¡¨
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        start_frame = group[0]
        end_frame = group[-1]
        
        # ç»Ÿè®¡ä¿¡æ¯
        group_distances = [self.distances[idx] for idx in group]
        max_jump = max(group_distances)
        min_jump = min(group_distances)
        avg_jump = np.mean(group_distances)
        
        # è·å–å‰åä¸Šä¸‹æ–‡ï¼ˆÂ±5å¸§ï¼‰
        context_start = max(0, start_frame - 5)
        context_end = min(len(self.distances), end_frame + 6)
        context_distances = self.distances[context_start:context_end].tolist()
        
        # è®¡ç®—ä½ç½®å˜åŒ–
        pos_start = self.positions[start_frame]
        pos_end = self.positions[end_frame + 1]  # +1å› ä¸ºdistanceæ˜¯diff
        total_displacement = np.linalg.norm(pos_end - pos_start)
        
        # åˆ¤æ–­æ˜¯å¦åœ¨å­åœ°å›¾è¾¹ç•Œï¼ˆå‡è®¾æ¯690å¸§ä¸€ä¸ªå­åœ°å›¾ï¼‰
        nearest_boundary = round(start_frame / 690) * 690
        distance_to_boundary = abs(start_frame - nearest_boundary)
        at_boundary = distance_to_boundary <= 50
        
        return {
            'start_frame': start_frame,
            'end_frame': end_frame,
            'span': end_frame - start_frame + 1,
            'num_jumps': len(group),
            'max_jump': max_jump,
            'min_jump': min_jump,
            'avg_jump': avg_jump,
            'total_displacement': total_displacement,
            'context_start': context_start,
            'context_end': context_end,
            'context_distances': context_distances,
            'nearest_boundary': nearest_boundary,
            'distance_to_boundary': distance_to_boundary,
            'at_boundary': at_boundary,
            'jump_details': [(idx, self.distances[idx]) for idx in group]
        }
    
    def generate_report(self, output_file: Path = None) -> str:
        """
        ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æŠ¥å‘Šæ–‡æœ¬
        """
        lines = []
        
        # æ ‡é¢˜
        lines.append("=" * 100)
        lines.append(" " * 30 + "ğŸ” ç›¸æœºè½¨è¿¹çªè·³åˆ†ææŠ¥å‘Š")
        lines.append("=" * 100)
        lines.append("")
        
        # åŸºæœ¬ä¿¡æ¯
        lines.append("ğŸ“‹ åŸºæœ¬ä¿¡æ¯")
        lines.append("-" * 100)
        lines.append(f"æ–‡ä»¶è·¯å¾„: {self.pose_file}")
        lines.append(f"æ€»å¸§æ•°: {len(self.positions)}")
        lines.append(f"æ€»è¡Œç¨‹: {np.sum(self.distances):.2f} ç±³")
        lines.append("")
        
        # ç»Ÿè®¡ä¿¡æ¯
        lines.append("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
        lines.append("-" * 100)
        lines.append(f"å¹³å‡å¸§é—´è·: {self.mean_dist:.4f} ç±³")
        lines.append(f"æ ‡å‡†å·®: {self.std_dist:.4f} ç±³")
        lines.append(f"æœ€å¤§å¸§é—´è·: {np.max(self.distances):.4f} ç±³")
        lines.append(f"æœ€å°å¸§é—´è·: {np.min(self.distances):.4f} ç±³")
        lines.append(f"çªè·³é˜ˆå€¼ (å‡å€¼+{self.sigma_multiplier}Ïƒ): {self.threshold:.4f} ç±³")
        lines.append("")
        
        # çªè·³æ¦‚è§ˆ
        lines.append("âš ï¸  çªè·³æ¦‚è§ˆ")
        lines.append("-" * 100)
        lines.append(f"çªè·³æ€»æ•°: {len(self.jump_indices)} ä¸ª ({len(self.jump_indices)/len(self.distances)*100:.2f}%)")
        
        if len(self.jump_indices) == 0:
            lines.append("\nâœ… æœªå‘ç°çªè·³ï¼Œè½¨è¿¹éå¸¸å¹³æ»‘ï¼")
            report = "\n".join(lines)
            if output_file:
                output_file.write_text(report)
                print(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
            return report
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åºçš„å‰10ä¸ªçªè·³
        top_jumps = sorted(enumerate(self.distances), key=lambda x: x[1], reverse=True)[:10]
        lines.append(f"\næœ€ä¸¥é‡çš„10ä¸ªçªè·³:")
        lines.append(f"  {'å¸§ç´¢å¼•':<12} {'è·³å˜è·ç¦»':<15} {'ä½ç½®':<40}")
        lines.append(f"  {'-'*70}")
        
        for idx, dist in top_jumps:
            if dist <= self.threshold:
                break
            pos = self.positions[idx]
            lines.append(f"  å¸§{idx:<9} {dist:>8.2f} ç±³       ({pos[0]:.2f}, {pos[1]:.2f}, {pos[2]:.2f})")
        
        lines.append("")
        
        # çªè·³èšé›†åŒºåŸŸåˆ†æ
        jump_groups = self.group_consecutive_jumps(gap=50)
        lines.append(f"ğŸ”¥ å‘ç° {len(jump_groups)} ä¸ªçªè·³èšé›†åŒºåŸŸ")
        lines.append("=" * 100)
        
        # æŒ‰æœ€å¤§è·³å˜æ’åº
        sorted_groups = sorted(jump_groups, 
                              key=lambda g: max(self.distances[idx] for idx in g), 
                              reverse=True)
        
        for i, group in enumerate(sorted_groups, 1):
            info = self.analyze_jump_group(group)
            
            lines.append("")
            lines.append(f"åŒºåŸŸ {i}: å¸§ {info['start_frame']} - {info['end_frame']}")
            lines.append("-" * 100)
            lines.append(f"  è·¨åº¦: {info['span']} å¸§")
            lines.append(f"  çªè·³æ•°é‡: {info['num_jumps']} ä¸ª")
            lines.append(f"  æœ€å¤§è·³å˜: {info['max_jump']:.2f} ç±³")
            lines.append(f"  æœ€å°è·³å˜: {info['min_jump']:.2f} ç±³")
            lines.append(f"  å¹³å‡è·³å˜: {info['avg_jump']:.2f} ç±³")
            lines.append(f"  æ€»ä½ç§»: {info['total_displacement']:.2f} ç±³")
            
            # ä½ç½®ä¿¡æ¯
            if info['at_boundary']:
                boundary_num = info['nearest_boundary'] // 690
                lines.append(f"  ğŸ“ ä½ç½®: é è¿‘å­åœ°å›¾è¾¹ç•Œ{boundary_num} (è·ç¦»è¾¹ç•Œ{info['distance_to_boundary']}å¸§)")
            else:
                boundary_num = info['nearest_boundary'] // 690
                lines.append(f"  ğŸ“ ä½ç½®: è¿œç¦»è¾¹ç•Œ (æœ€è¿‘è¾¹ç•Œ{boundary_num}ï¼Œè·ç¦»{info['distance_to_boundary']}å¸§)")
            
            # ä¸¥é‡åº¦è¯„çº§
            if info['max_jump'] > 25:
                severity = "ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ æå…¶ä¸¥é‡"
            elif info['max_jump'] > 15:
                severity = "ğŸ”´ğŸ”´ğŸ”´ éå¸¸ä¸¥é‡"
            elif info['max_jump'] > 10:
                severity = "ğŸŸ ğŸŸ  ä¸¥é‡"
            elif info['max_jump'] > 5:
                severity = "ğŸŸ  ä¸­ç­‰"
            else:
                severity = "ğŸŸ¡ è½»å¾®"
            lines.append(f"  âš ï¸  ä¸¥é‡åº¦: {severity}")
            
            # è¯¦ç»†çªè·³åˆ—è¡¨
            lines.append(f"  çªè·³è¯¦æƒ…:")
            for j, (idx, dist) in enumerate(info['jump_details'][:10], 1):
                lines.append(f"    å¸§{idx}: {dist:.2f}ç±³")
                if j == 10 and len(info['jump_details']) > 10:
                    lines.append(f"    ... (è¿˜æœ‰{len(info['jump_details'])-10}ä¸ª)")
                    break
            
            # å‰åä¸Šä¸‹æ–‡
            lines.append(f"  å‰åå¸§è·ç¦» (å¸§{info['context_start']}-{info['context_end']}):")
            for k, ctx_idx in enumerate(range(info['context_start'], info['context_end'])):
                if ctx_idx < len(self.distances):
                    dist = self.distances[ctx_idx]
                    is_jump = ctx_idx in group
                    marker = "ğŸ’¥" if is_jump else "  "
                    lines.append(f"    å¸§{ctx_idx}: {dist:.2f}ç±³ {marker}")
        
        # å»ºè®®
        lines.append("")
        lines.append("=" * 100)
        lines.append("ğŸ’¡ åˆ†æä¸å»ºè®®")
        lines.append("=" * 100)
        
        # ç»Ÿè®¡è¾¹ç•Œvséè¾¹ç•Œ
        groups_at_boundary = sum(1 for g in jump_groups 
                                if self.analyze_jump_group(g)['at_boundary'])
        groups_not_at_boundary = len(jump_groups) - groups_at_boundary
        
        lines.append(f"\nçªè·³åˆ†å¸ƒ:")
        lines.append(f"  â€¢ å­åœ°å›¾è¾¹ç•Œå¤„: {groups_at_boundary} ä¸ªåŒºåŸŸ")
        lines.append(f"  â€¢ å­åœ°å›¾å†…éƒ¨: {groups_not_at_boundary} ä¸ªåŒºåŸŸ")
        
        if groups_at_boundary > groups_not_at_boundary:
            lines.append(f"\nâš ï¸  å¤§éƒ¨åˆ†çªè·³åœ¨å­åœ°å›¾è¾¹ç•Œï¼Œå»ºè®®:")
            lines.append(f"  1. æ£€æŸ¥å­åœ°å›¾å¯¹é½ç®—æ³•")
            lines.append(f"  2. è°ƒæ•´RANSACå‚æ•°æˆ–å¯ç”¨ä½å§¿èåˆ")
            lines.append(f"  3. å¢åŠ é‡å åŒºåŸŸå¸§æ•°")
        else:
            lines.append(f"\nâš ï¸  å¤§éƒ¨åˆ†çªè·³åœ¨å­åœ°å›¾å†…éƒ¨ï¼Œå»ºè®®:")
            lines.append(f"  1. æ£€æŸ¥åŸå§‹SLAMè½¨è¿¹è´¨é‡")
            lines.append(f"  2. æŸ¥çœ‹é—®é¢˜åŒºåŸŸçš„åŸå§‹å›¾åƒ")
            lines.append(f"  3. è€ƒè™‘é‡æ–°è¿è¡ŒSLAMï¼Œè°ƒæ•´å‚æ•°")
            lines.append(f"  4. ä½¿ç”¨IMUæˆ–å…¶ä»–ä¼ æ„Ÿå™¨è¾…åŠ©")
        
        # æœ€ä¸¥é‡åŒºåŸŸè­¦å‘Š
        worst_group = sorted_groups[0]
        worst_info = self.analyze_jump_group(worst_group)
        lines.append(f"\nğŸš¨ æœ€ä¸¥é‡åŒºåŸŸè­¦å‘Š:")
        lines.append(f"  å¸§{worst_info['start_frame']}-{worst_info['end_frame']}: æœ€å¤§è·³å˜{worst_info['max_jump']:.2f}ç±³")
        lines.append(f"  è¿™ä¸ªåŒºåŸŸéœ€è¦ä¼˜å…ˆå¤„ç†ï¼")
        
        lines.append("")
        lines.append("=" * 100)
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡æœ¬
        report = "\n".join(lines)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_file:
            output_file.write_text(report)
            print(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
        return report
    
    def export_jump_list(self, output_file: Path):
        """
        å¯¼å‡ºçªè·³åˆ—è¡¨ä¸ºCSVæ ¼å¼
        
        Args:
            output_file: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
        """
        import csv
        
        with open(output_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Frame_Index', 'Jump_Distance_m', 'Position_X', 'Position_Y', 'Position_Z', 
                           'At_Boundary', 'Nearest_Boundary', 'Distance_to_Boundary'])
            
            for idx in self.jump_indices:
                dist = self.distances[idx]
                pos = self.positions[idx]
                
                nearest_boundary = round(idx / 690) * 690
                distance_to_boundary = abs(idx - nearest_boundary)
                at_boundary = distance_to_boundary <= 50
                
                writer.writerow([
                    idx, f"{dist:.4f}", 
                    f"{pos[0]:.4f}", f"{pos[1]:.4f}", f"{pos[2]:.4f}",
                    at_boundary, nearest_boundary, distance_to_boundary
                ])
        
        print(f"ğŸ“Š çªè·³åˆ—è¡¨å·²å¯¼å‡ºåˆ°: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description="åˆ†æç›¸æœºä½å§¿è½¨è¿¹ä¸­çš„çªè·³",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  åŸºæœ¬ç”¨æ³•:
    python analyze_jumps.py data1/camera_poses.json
  
  è‡ªå®šä¹‰é˜ˆå€¼ (4Ïƒ):
    python analyze_jumps.py data1/camera_poses.json --sigma 4.0
  
  ä¿å­˜æŠ¥å‘Š:
    python analyze_jumps.py data1/camera_poses.json -o report.txt
  
  å¯¼å‡ºCSV:
    python analyze_jumps.py data1/camera_poses.json --csv jumps.csv
        """
    )
    
    parser.add_argument(
        'pose_file',
        type=Path,
        help='camera_poses.jsonæ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '-o', '--output',
        type=Path,
        default=None,
        help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ (é»˜è®¤: åªæ‰“å°åˆ°ç»ˆç«¯)'
    )
    
    parser.add_argument(
        '--csv',
        type=Path,
        default=None,
        help='å¯¼å‡ºçªè·³åˆ—è¡¨ä¸ºCSVæ–‡ä»¶'
    )
    
    parser.add_argument(
        '--sigma',
        type=float,
        default=3.0,
        help='çªè·³é˜ˆå€¼çš„æ ‡å‡†å·®å€æ•° (é»˜è®¤: 3.0ï¼Œå³3Ïƒ)'
    )
    
    parser.add_argument(
        '--threshold',
        type=float,
        default=None,
        help='ç›´æ¥æŒ‡å®šçªè·³é˜ˆå€¼ï¼ˆç±³ï¼‰ï¼Œè¦†ç›–--sigmaå‚æ•°'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not args.pose_file.exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {args.pose_file}")
        return 1
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = JumpAnalyzer(args.pose_file, sigma_multiplier=args.sigma)
    
    # å¦‚æœæŒ‡å®šäº†é˜ˆå€¼ï¼Œè¦†ç›–è‡ªåŠ¨è®¡ç®—çš„é˜ˆå€¼
    if args.threshold is not None:
        analyzer.threshold = args.threshold
        analyzer.jump_mask = analyzer.distances > analyzer.threshold
        analyzer.jump_indices = np.where(analyzer.jump_mask)[0]
        print(f"ğŸ¯ ä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼: {analyzer.threshold:.4f}m")
        print(f"âš ï¸  é‡æ–°è¯†åˆ«åˆ° {len(analyzer.jump_indices)} ä¸ªçªè·³\n")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_report(output_file=args.output)
    
    # æ‰“å°åˆ°ç»ˆç«¯ï¼ˆå¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼‰
    if args.output is None:
        print(report)
    
    # å¯¼å‡ºCSV
    if args.csv:
        analyzer.export_jump_list(args.csv)
    
    return 0


if __name__ == '__main__':
    exit(main())
